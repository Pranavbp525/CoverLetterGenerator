{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_applicant_data(yaml_file_path):\n",
    "    with open(yaml_file_path, 'r') as file:\n",
    "        applicant_data = yaml.safe_load(file)\n",
    "    return applicant_data\n",
    "\n",
    "applicant_data = load_applicant_data('/Users/pranavtandra/Desktop/CoverLetterGenerator/resume.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'personal_information': {'name': 'Bhavya Pranav',\n",
       "  'surname': 'Tandra',\n",
       "  'date_of_birth': '25/12/2000',\n",
       "  'country': 'United States of America',\n",
       "  'city': 'Boston',\n",
       "  'address': '75 Saint Alphonsus Street, Roxbury Crossing, Boston, MA, 02120',\n",
       "  'phone_prefix': '+1',\n",
       "  'phone': '6179062441',\n",
       "  'email': 'tandra.b@northeastern.edu',\n",
       "  'github': 'https://github.com/Pranavbp525',\n",
       "  'linkedin': 'https://www.linkedin.com/in/pranav-tandra525/'},\n",
       " 'education_details': [{'degree': 'Masters',\n",
       "   'university': 'Northeastern University',\n",
       "   'gpa': '3.76/4.00',\n",
       "   'graduation_year': '2025',\n",
       "   'field_of_study': 'Artificial Intelligence',\n",
       "   'courses': ['Large Language Models',\n",
       "    'Deep Learning',\n",
       "    'Machine Learning',\n",
       "    'Foundations in Artificial Intelligence',\n",
       "    'Artificial Intelligence for Human Computer Interaction',\n",
       "    'Algorithms',\n",
       "    'Programming Design Paradigm']},\n",
       "  {'degree': 'Bachelors',\n",
       "   'university': 'Malla Reddy College of Engineering and Technology',\n",
       "   'gpa': '8.43/10.00',\n",
       "   'graduation_year': '2022',\n",
       "   'field_of_study': 'Computer Science',\n",
       "   'courses': ['Data Warehousing and Data Mining',\n",
       "    'Cloud Computing',\n",
       "    'Linux',\n",
       "    'Big Data Analytics',\n",
       "    'Data Visualization',\n",
       "    'Parallel and Distributed Computing',\n",
       "    'Database Management Systems']}],\n",
       " 'experience_details': [{'position': 'Senior Analyst (Data Engineer)',\n",
       "   'company': 'Capgemini',\n",
       "   'employment_period': '07/2022 - 12/2022',\n",
       "   'location': 'India',\n",
       "   'key_responsibilities': ['Architected and implemented a data processing pipeline using AWS tools, integrating data from multiple sources into a unified structure. Utilized AWS S3 for storage, Glue for ETL, and Redshift for warehousing, ensuring accuracy and compliance.',\n",
       "    'Designed and executed multi-stage data workflows, including ingestion into S3, transformation with PySpark on AWS EMR, and integration into Redshift. Used AWS Athena for SQL querying and validation, enhancing quality and efficiency.',\n",
       "    'Developed and enforced data quality checks and error handling, leveraging AWS services to boost data integrity. Implemented automated monitoring and reporting with AWS MWAA, reducing manual interventions and improving reliability.'],\n",
       "   'skills_acquired': ['AWS',\n",
       "    'AWS S3',\n",
       "    'AWS Glue',\n",
       "    'AWS Redshift',\n",
       "    'AWS Elastic Map-Reduce',\n",
       "    'AWS Athena',\n",
       "    'AWS MWAA',\n",
       "    'PySpark']},\n",
       "  {'position': 'Data Engineering Intern',\n",
       "   'company': 'Capgemini',\n",
       "   'employment_period': '02/2022 - 05/2022',\n",
       "   'location': 'India',\n",
       "   'key_responsibilities': ['Engineered and optimized end-to-end data pipelines using Azure Data Factory and Azure DataBricks, processing a decade-long dataset. Utilized PySpark for scalable data transformations and integrated with ADLS Gen2 for efficient data storage.',\n",
       "    'Performed complex data manipulations and analysis with Azure SQL Database, applying advanced filtering and transformations to uncover trends and contributing factors, and used Azure Synapse Analytics for seamless data integration and querying.',\n",
       "    'Developed interactive dashboards and detailed reports using Power BI and Tableau, visualizing trends and key factors in the dataset.'],\n",
       "   'skills_acquired': ['Microsoft Azure',\n",
       "    'Azure Data Factory',\n",
       "    'Azure DataBricks',\n",
       "    'MS-SQL',\n",
       "    'ADLS Gen2',\n",
       "    'Azure SQL Database',\n",
       "    'Azure Synapse Analytics',\n",
       "    'Power BI',\n",
       "    'Tableu']}],\n",
       " 'projects': [{'name': 'Google’s Isolated American Sign Language Recognition',\n",
       "   'description': [\"Utilized multiple models, including Transformer Encoder, LSTM, GRU, RNN, and traditional machine learning models such as SVM and KNN, to classify Mediapipe Landmark data files from video streams into 250 distinct 'sign' classes.\",\n",
       "    'Conducted a comprehensive comparative analysis to evaluate the performance of each model in action recognition',\n",
       "    'Conducted extensive hyperparameter tuning using Optuna across a diverse range of values for all models, ensuring each model was trained with its optimal hyperparameters.',\n",
       "    'Executed thorough data cleaning and processing procedures to significantly reduce the dataset size from 56GB to 12GB to facilitate model training within the constraints of limited available resources, ensuring a more streamlined and resource-efficient training process.']},\n",
       "  {'name': 'Image Restoration using Multi Stage Progressive Image Restoration Model',\n",
       "   'description': ['Revitalized images by addressing a spectrum of degradations, spanning from noise and motion-blur to rain artifacts, employing the state-of-the-art MPRNET model.',\n",
       "    'Leveraged specific datasets, including GoPro for deblurring, SIDD for denoising, and synthetic rain datasets for deraining, while implementing a diverse set of data augmentations, such as random cropping, flipping, rotating, and adjustments to gamma and saturation, effectively expanding the dataset for more robust model training.',\n",
       "    \"Engineered a sophisticated 3-stage network architecture, with the initial two stages adopting a U-Net structure to grasp contextual information, and the third stage meticulously designed to preserve the original resolution of the image, ensuring the retention of intricate details throughout the training process, thereby maximizing the model's capacity to restore images with enhanced fidelity and clarity across various degradation scenarios.\"]},\n",
       "  {'name': 'ImageManipulation Application using JAVA',\n",
       "   'description': ['Developed a comprehensive Image Manipulation software with blur, filter, sepia, greyscale and more abilites exclusively using fundamental JDK components.',\n",
       "    'Expertly applied design patterns, including the builder and command design patterns, and diligently followed MVC principles to create a highly scalable application while strictly adhering to SOLID principles.',\n",
       "    'Emphasized modularity and scalability without compromising functionality or code readability throughout the project.']},\n",
       "  {'name': 'Celeb Face Generator',\n",
       "   'description': ['Implemented a complex Denoising Diffusion Probabilistic Model, as described in the prominent research paper, using PyTorch.',\n",
       "    'Trained the model on the CelebAHQ Dataset to generate new, high-quality images.',\n",
       "    'Demonstrated advanced skills in Python, machine learning, and image processing techniques.',\n",
       "    'Successfully created a model capable of generating realistic human face images, showcasing the potential in generative modeling and AI-driven image synthesis.']},\n",
       "  {'name': 'English to Italian Translation using Transformer',\n",
       "   'description': ['Developed a Transformer model from scratch for machine translation based on ”Attention is All You Need” by Vaswani et al., utilizing self-attention mechanisms for efficient language translation.',\n",
       "    \"Enabled easy customization for various language pairs through a configurable system, enhancing the model's adaptability for different linguistic datasets.\",\n",
       "    \"Utilized PyTorch for all aspects of the model's lifecycle, including construction, training, and inference, leveraging GPU acceleration for improved performance.\",\n",
       "    'Implemented a greedy decoding strategy for translation, providing a straightforward and effective approach for text translation from source to target languages.',\n",
       "    'Designed a dynamic bilingual dataset loader to facilitate the efficient loading and preprocessing of bilingual sentence pairs, optimizing for computational efficiency and memory usage.']},\n",
       "  {'name': 'Fine-tuning Seq2Seq Models for Text Summarization on Conversational Data',\n",
       "   'description': ['Implemented a text summarization pipeline using the Samsum Corpus, focusing on generating concise summaries from conversational text.',\n",
       "    \"Conducted Exploratory Data Analysis (EDA) to understand dataset characteristics, including dialogue and summary lengths, using Python's pandas and matplotlib libraries.\",\n",
       "    'Utilized pre-trained models (BART, T5, and Pegasus) for initial summarization attempts, identifying limitations in handling conversational data.',\n",
       "    'Fine-tuned the BART-large-CNN model on the Samsum Corpus to improve summarization performance, adapting the model to better understand conversational semantics.',\n",
       "    'Evaluated model performance using ROUGE scores, demonstrating significant improvement in summarization quality post fine-tuning.',\n",
       "    'Conducted both quantitative and qualitative analyses to compare pre-trained and fine-tuned model outputs, showcasing the enhanced coherence and relevance of generated summaries.']},\n",
       "  {'name': 'Generating High Quality Stories by Fine-Tuning and Aligning Phi-2',\n",
       "   'description': ['Pioneering the development of StoryGen, a novel Small Language Model based on Phi-2, aimed at revolutionizing storytelling by employing unique fine-tuning and instruct-tuning techniques on a diverse Stories Dataset to capture intricate narrative nuances.',\n",
       "    \"Leveraging advanced AI techniques, including QLoRA for Parameter Efficient Fine-Tuning, to refine the model's capacity for generating compelling narratives that resonate deeply with audiences.\",\n",
       "    \"Implemented Reinforcement Learning from Human Feedback (RLHF) to significantly elevate story quality, coherence, and narrative complexity, showcasing potential through preliminary human evaluators' ratings and user engagement metrics.\",\n",
       "    'Innovated with alignment techniques such as Direct Preference Optimization (DPO) and Self-Rewarding Language Models, to align the generated stories closely with human preferences and storytelling standards.',\n",
       "    \"Conducted comprehensive evaluations through a blend of human ratings and quantitative metrics, aiming to demonstrate StoryGen's superior narrative generation capabilities compared to conventional models.\",\n",
       "    \"Used HuggingFace's Transformers, TRL, PEFT and BitsAndBytes, Accelerate and DeepSpeed.\"]},\n",
       "  {'name': 'Lightweight Visual Question Answering using Gemma2B-it',\n",
       "   'description': ['Spearheading the development of a Lightweight Visual Question Answering (LVQA) system, leveraging the Gemma 2B Instruct model, into LLaVA framework to optimize efficiency and scalability.',\n",
       "    'Employing advanced natural language processing and image encoding techniques, integrating CLIP-ViT-L-336px with Gemma2B-it, to refine visual question interpretation capabilities.',\n",
       "    'Pretraining multimodal Large Language and Vision model on text-image pairs, and Visual Instruction Tuning on synthetic Visual Instruct Data.',\n",
       "    'Utilizing state-of-the-art tools such as PyTorch, TensorFlow, and Hugging Face Transformers to facilitate model development and performance benchmarking.',\n",
       "    \"Benchmark evaluations highlight the LVQA system's potential to surpass existing models in AI efficiency and accessibility, indicating a groundbreaking step forward in the field.\"]}],\n",
       " 'achievements': [{'name': 'Most Job Offers',\n",
       "   'description': 'Awarded for securing the highest number of job offers in undergraduate college.'},\n",
       "  {'name': 'Code Debugging Winner',\n",
       "   'description': \"Won the code-debugging competition in undergraduate college's tech fest.\"}],\n",
       " 'certifications': ['Coursera- Deep Learning Specialization',\n",
       "  'Coursera- AI for Medicine Specialization',\n",
       "  'IBM Data Visualization',\n",
       "  'Google Data Analytics Professional Certificate',\n",
       "  'Coursera- MLOps Specialization by Duke University'],\n",
       " 'skills': {'programing_languages': ['Java',\n",
       "   'Python',\n",
       "   'C',\n",
       "   'C++',\n",
       "   'R',\n",
       "   'MATLAB'],\n",
       "  'databases': ['MongoDB',\n",
       "   'ElasticSearch',\n",
       "   'Firebase',\n",
       "   'MySQL',\n",
       "   'Cassandra',\n",
       "   'Oracle',\n",
       "   'PostgreSQL',\n",
       "   'MSSQL',\n",
       "   'SQLite',\n",
       "   'Amazon DynamoDB',\n",
       "   'ChromaDB',\n",
       "   'Pinecone',\n",
       "   'FAISS'],\n",
       "  'frameworks': ['PyTorch',\n",
       "   'TensorFlow',\n",
       "   'Keras',\n",
       "   'HuggingFace',\n",
       "   'Langchain',\n",
       "   'OpenAI',\n",
       "   'VertexAI',\n",
       "   'OpenCV',\n",
       "   'NLTK',\n",
       "   'Scikit-Learn',\n",
       "   'NumPy',\n",
       "   'Pandas',\n",
       "   'SciPy',\n",
       "   'PySpark'],\n",
       "  'tools': ['Git',\n",
       "   'MLFlow',\n",
       "   'DVC',\n",
       "   'AirFlow',\n",
       "   'Docker',\n",
       "   'GitHub Actions',\n",
       "   'Kubernetes',\n",
       "   'Grafana',\n",
       "   'Terraform'],\n",
       "  'cloud': ['Amazon Sagemaker',\n",
       "   'AWS Lambda',\n",
       "   'AWS EMR',\n",
       "   'AWS S3',\n",
       "   'AWS Redshift',\n",
       "   'Amazon RDS',\n",
       "   'Amazon Cloudwatch',\n",
       "   'Amazon EC2',\n",
       "   'Amazon ECS',\n",
       "   'Amazon EKS',\n",
       "   'Azure DevOps',\n",
       "   'Azure MachineLearning',\n",
       "   'Azure DataBricks',\n",
       "   'Azure Synapse Analytics',\n",
       "   'Azure Data Lake',\n",
       "   'Azure Functions',\n",
       "   'Azure Kubernetes Services',\n",
       "   'Azure Event Hubs',\n",
       "   'Azure Cognitive Servieces']}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applicant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['personal_information', 'education_details', 'experience_details', 'projects', 'achievements', 'certifications', 'skills'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applicant_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Bhavya Pranav',\n",
       " 'surname': 'Tandra',\n",
       " 'date_of_birth': '25/12/2000',\n",
       " 'country': 'United States of America',\n",
       " 'city': 'Boston',\n",
       " 'address': '75 Saint Alphonsus Street, Roxbury Crossing, Boston, MA, 02120',\n",
       " 'phone_prefix': '+1',\n",
       " 'phone': '6179062441',\n",
       " 'email': 'tandra.b@northeastern.edu',\n",
       " 'github': 'https://github.com/Pranavbp525',\n",
       " 'linkedin': 'https://www.linkedin.com/in/pranav-tandra525/'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applicant_data['personal_information']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'degree': 'Masters',\n",
       "  'university': 'Northeastern University',\n",
       "  'gpa': '3.76/4.00',\n",
       "  'graduation_year': '2025',\n",
       "  'field_of_study': 'Artificial Intelligence',\n",
       "  'courses': ['Large Language Models',\n",
       "   'Deep Learning',\n",
       "   'Machine Learning',\n",
       "   'Foundations in Artificial Intelligence',\n",
       "   'Artificial Intelligence for Human Computer Interaction',\n",
       "   'Algorithms',\n",
       "   'Programming Design Paradigm']},\n",
       " {'degree': 'Bachelors',\n",
       "  'university': 'Malla Reddy College of Engineering and Technology',\n",
       "  'gpa': '8.43/10.00',\n",
       "  'graduation_year': '2022',\n",
       "  'field_of_study': 'Computer Science',\n",
       "  'courses': ['Data Warehousing and Data Mining',\n",
       "   'Cloud Computing',\n",
       "   'Linux',\n",
       "   'Big Data Analytics',\n",
       "   'Data Visualization',\n",
       "   'Parallel and Distributed Computing',\n",
       "   'Database Management Systems']}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applicant_data['education_details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'position': 'Senior Analyst (Data Engineer)',\n",
       "  'company': 'Capgemini',\n",
       "  'employment_period': '07/2022 - 12/2022',\n",
       "  'location': 'India',\n",
       "  'key_responsibilities': ['Architected and implemented a data processing pipeline using AWS tools, integrating data from multiple sources into a unified structure. Utilized AWS S3 for storage, Glue for ETL, and Redshift for warehousing, ensuring accuracy and compliance.',\n",
       "   'Designed and executed multi-stage data workflows, including ingestion into S3, transformation with PySpark on AWS EMR, and integration into Redshift. Used AWS Athena for SQL querying and validation, enhancing quality and efficiency.',\n",
       "   'Developed and enforced data quality checks and error handling, leveraging AWS services to boost data integrity. Implemented automated monitoring and reporting with AWS MWAA, reducing manual interventions and improving reliability.'],\n",
       "  'skills_acquired': ['AWS',\n",
       "   'AWS S3',\n",
       "   'AWS Glue',\n",
       "   'AWS Redshift',\n",
       "   'AWS Elastic Map-Reduce',\n",
       "   'AWS Athena',\n",
       "   'AWS MWAA',\n",
       "   'PySpark']},\n",
       " {'position': 'Data Engineering Intern',\n",
       "  'company': 'Capgemini',\n",
       "  'employment_period': '02/2022 - 05/2022',\n",
       "  'location': 'India',\n",
       "  'key_responsibilities': ['Engineered and optimized end-to-end data pipelines using Azure Data Factory and Azure DataBricks, processing a decade-long dataset. Utilized PySpark for scalable data transformations and integrated with ADLS Gen2 for efficient data storage.',\n",
       "   'Performed complex data manipulations and analysis with Azure SQL Database, applying advanced filtering and transformations to uncover trends and contributing factors, and used Azure Synapse Analytics for seamless data integration and querying.',\n",
       "   'Developed interactive dashboards and detailed reports using Power BI and Tableau, visualizing trends and key factors in the dataset.'],\n",
       "  'skills_acquired': ['Microsoft Azure',\n",
       "   'Azure Data Factory',\n",
       "   'Azure DataBricks',\n",
       "   'MS-SQL',\n",
       "   'ADLS Gen2',\n",
       "   'Azure SQL Database',\n",
       "   'Azure Synapse Analytics',\n",
       "   'Power BI',\n",
       "   'Tableu']}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applicant_data['experience_details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Google’s Isolated American Sign Language Recognition',\n",
       "  'description': [\"Utilized multiple models, including Transformer Encoder, LSTM, GRU, RNN, and traditional machine learning models such as SVM and KNN, to classify Mediapipe Landmark data files from video streams into 250 distinct 'sign' classes.\",\n",
       "   'Conducted a comprehensive comparative analysis to evaluate the performance of each model in action recognition',\n",
       "   'Conducted extensive hyperparameter tuning using Optuna across a diverse range of values for all models, ensuring each model was trained with its optimal hyperparameters.',\n",
       "   'Executed thorough data cleaning and processing procedures to significantly reduce the dataset size from 56GB to 12GB to facilitate model training within the constraints of limited available resources, ensuring a more streamlined and resource-efficient training process.']},\n",
       " {'name': 'Image Restoration using Multi Stage Progressive Image Restoration Model',\n",
       "  'description': ['Revitalized images by addressing a spectrum of degradations, spanning from noise and motion-blur to rain artifacts, employing the state-of-the-art MPRNET model.',\n",
       "   'Leveraged specific datasets, including GoPro for deblurring, SIDD for denoising, and synthetic rain datasets for deraining, while implementing a diverse set of data augmentations, such as random cropping, flipping, rotating, and adjustments to gamma and saturation, effectively expanding the dataset for more robust model training.',\n",
       "   \"Engineered a sophisticated 3-stage network architecture, with the initial two stages adopting a U-Net structure to grasp contextual information, and the third stage meticulously designed to preserve the original resolution of the image, ensuring the retention of intricate details throughout the training process, thereby maximizing the model's capacity to restore images with enhanced fidelity and clarity across various degradation scenarios.\"]},\n",
       " {'name': 'ImageManipulation Application using JAVA',\n",
       "  'description': ['Developed a comprehensive Image Manipulation software with blur, filter, sepia, greyscale and more abilites exclusively using fundamental JDK components.',\n",
       "   'Expertly applied design patterns, including the builder and command design patterns, and diligently followed MVC principles to create a highly scalable application while strictly adhering to SOLID principles.',\n",
       "   'Emphasized modularity and scalability without compromising functionality or code readability throughout the project.']},\n",
       " {'name': 'Celeb Face Generator',\n",
       "  'description': ['Implemented a complex Denoising Diffusion Probabilistic Model, as described in the prominent research paper, using PyTorch.',\n",
       "   'Trained the model on the CelebAHQ Dataset to generate new, high-quality images.',\n",
       "   'Demonstrated advanced skills in Python, machine learning, and image processing techniques.',\n",
       "   'Successfully created a model capable of generating realistic human face images, showcasing the potential in generative modeling and AI-driven image synthesis.']},\n",
       " {'name': 'English to Italian Translation using Transformer',\n",
       "  'description': ['Developed a Transformer model from scratch for machine translation based on ”Attention is All You Need” by Vaswani et al., utilizing self-attention mechanisms for efficient language translation.',\n",
       "   \"Enabled easy customization for various language pairs through a configurable system, enhancing the model's adaptability for different linguistic datasets.\",\n",
       "   \"Utilized PyTorch for all aspects of the model's lifecycle, including construction, training, and inference, leveraging GPU acceleration for improved performance.\",\n",
       "   'Implemented a greedy decoding strategy for translation, providing a straightforward and effective approach for text translation from source to target languages.',\n",
       "   'Designed a dynamic bilingual dataset loader to facilitate the efficient loading and preprocessing of bilingual sentence pairs, optimizing for computational efficiency and memory usage.']},\n",
       " {'name': 'Fine-tuning Seq2Seq Models for Text Summarization on Conversational Data',\n",
       "  'description': ['Implemented a text summarization pipeline using the Samsum Corpus, focusing on generating concise summaries from conversational text.',\n",
       "   \"Conducted Exploratory Data Analysis (EDA) to understand dataset characteristics, including dialogue and summary lengths, using Python's pandas and matplotlib libraries.\",\n",
       "   'Utilized pre-trained models (BART, T5, and Pegasus) for initial summarization attempts, identifying limitations in handling conversational data.',\n",
       "   'Fine-tuned the BART-large-CNN model on the Samsum Corpus to improve summarization performance, adapting the model to better understand conversational semantics.',\n",
       "   'Evaluated model performance using ROUGE scores, demonstrating significant improvement in summarization quality post fine-tuning.',\n",
       "   'Conducted both quantitative and qualitative analyses to compare pre-trained and fine-tuned model outputs, showcasing the enhanced coherence and relevance of generated summaries.']},\n",
       " {'name': 'Generating High Quality Stories by Fine-Tuning and Aligning Phi-2',\n",
       "  'description': ['Pioneering the development of StoryGen, a novel Small Language Model based on Phi-2, aimed at revolutionizing storytelling by employing unique fine-tuning and instruct-tuning techniques on a diverse Stories Dataset to capture intricate narrative nuances.',\n",
       "   \"Leveraging advanced AI techniques, including QLoRA for Parameter Efficient Fine-Tuning, to refine the model's capacity for generating compelling narratives that resonate deeply with audiences.\",\n",
       "   \"Implemented Reinforcement Learning from Human Feedback (RLHF) to significantly elevate story quality, coherence, and narrative complexity, showcasing potential through preliminary human evaluators' ratings and user engagement metrics.\",\n",
       "   'Innovated with alignment techniques such as Direct Preference Optimization (DPO) and Self-Rewarding Language Models, to align the generated stories closely with human preferences and storytelling standards.',\n",
       "   \"Conducted comprehensive evaluations through a blend of human ratings and quantitative metrics, aiming to demonstrate StoryGen's superior narrative generation capabilities compared to conventional models.\",\n",
       "   \"Used HuggingFace's Transformers, TRL, PEFT and BitsAndBytes, Accelerate and DeepSpeed.\"]},\n",
       " {'name': 'Lightweight Visual Question Answering using Gemma2B-it',\n",
       "  'description': ['Spearheading the development of a Lightweight Visual Question Answering (LVQA) system, leveraging the Gemma 2B Instruct model, into LLaVA framework to optimize efficiency and scalability.',\n",
       "   'Employing advanced natural language processing and image encoding techniques, integrating CLIP-ViT-L-336px with Gemma2B-it, to refine visual question interpretation capabilities.',\n",
       "   'Pretraining multimodal Large Language and Vision model on text-image pairs, and Visual Instruction Tuning on synthetic Visual Instruct Data.',\n",
       "   'Utilizing state-of-the-art tools such as PyTorch, TensorFlow, and Hugging Face Transformers to facilitate model development and performance benchmarking.',\n",
       "   \"Benchmark evaluations highlight the LVQA system's potential to surpass existing models in AI efficiency and accessibility, indicating a groundbreaking step forward in the field.\"]}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applicant_data['projects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Most Job Offers',\n",
       "  'description': 'Awarded for securing the highest number of job offers in undergraduate college.'},\n",
       " {'name': 'Code Debugging Winner',\n",
       "  'description': \"Won the code-debugging competition in undergraduate college's tech fest.\"}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applicant_data['achievements']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coursera- Deep Learning Specialization',\n",
       " 'Coursera- AI for Medicine Specialization',\n",
       " 'IBM Data Visualization',\n",
       " 'Google Data Analytics Professional Certificate',\n",
       " 'Coursera- MLOps Specialization by Duke University']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applicant_data['certifications']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Java', 'Python', 'C', 'C++', 'R', 'MATLAB']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applicant_data['skills']['programing_languages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description=input(\"Enter your Job Description:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About the job Location: Remote Type: Internship Duration: 3-6 months Start Date: October 1st, 2024   About IAN: IAN (Intelligent Agent Network) is at the forefront of ethical AI innovation. By seamlessly connecting multiple AI agents, IAN provides clear, unbiased insights for businesses and individuals. Join us as we build a smarter, fairer AI ecosystem that drives real-world impact.   Role Overview: As an AI Engineer Intern at IAN, you will dive deep into the world of AI, working on multi-agent systems and advanced machine learning models. This role offers hands-on experience in AI development, focusing on integrating diverse AI agents and addressing biases in AI systems. If you’re passionate about ethical AI and eager to make a difference, this role is for you.   Key Responsibilities:  Collaborate with the engineering team to design, develop, and test AI models and algorithms. Assist in integrating multiple AI agents into a cohesive and powerful system. Conduct research on AI biases and help develop mitigation strategies. Work on data preprocessing, model training, and optimization of AI performance. Participate in code reviews and contribute to technical documentation. Present your work and findings to the team, showcasing the impact of your contributions. Qualifications:  Currently pursuing or recently completed a degree in Computer Science, Data Science, AI, Machine Learning, or a related field. Strong understanding of machine learning algorithms and AI concepts. Experience with programming languages such as Python, TensorFlow, PyTorch, or similar AI frameworks. Familiarity with data preprocessing, model training, and evaluation techniques. A passion for ethical AI and a drive to solve real-world problems using AI technology. Ability to work effectively in a fast-paced, collaborative environment. Bonus Skills:  Experience with natural language processing (NLP) models. Familiarity with cloud platforms (AWS, Google Cloud, etc.). Experience in hackathons, side projects, or launching a startup. Proven track record of building projects independently or as part of a team. Why Join IAN?  Gain hands-on experience in the ethical AI industry with real-world impact. Work alongside a dynamic team of engineers, data scientists, and AI researchers. Collaborate on meaningful projects with the potential to shape the future of AI. Flexible work hours with the opportunity to work remotely. Potential for full-time employment based on performance.   Salary  $120,000 USD   How to Apply: Submit your resume, a brief cover letter, and any relevant project work or portfolio to brett@cyphae.com. Applications are reviewed on a rolling basis, so we encourage you to apply early.   Join IAN and be a part of revolutionizing AI.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(applicant_data, job_description):\n",
    "    personal_info = f\"\"\"\n",
    "**Applicant's Personal Information:**\n",
    "Name: {applicant_data['personal_information']['name']} {applicant_data['personal_information']['surname']}\n",
    "Email: {applicant_data['personal_information']['email']}\n",
    "Phone Number: {applicant_data['personal_information']['phone_prefix']} {applicant_data['personal_information']['phone']}\n",
    "\"\"\"\n",
    "\n",
    "    education_details = f\"\"\"\n",
    "**Applicant's Education Details:**\n",
    "Education: {', '.join([edu['degree'] + ' in ' + edu['field_of_study'] + ' from ' + edu['university'] + ' with ' + edu['gpa'] + ' GPA' for edu in applicant_data['education_details']])}\n",
    "Course Work: {', '.join([', '.join(edu['courses']) for edu in applicant_data['education_details']])}\n",
    "\"\"\"\n",
    "\n",
    "    experience_details = \"\\n\".join([\n",
    "        f\"Worked as {exp['position']} at {exp['company']} where the job responsibilities include:\\n  - \" +\n",
    "        '\\n  - '.join(exp['key_responsibilities']) +\n",
    "        f\"\\n  - Skills Acquired: {', '.join(exp['skills_acquired'])}\\n\"\n",
    "        for exp in applicant_data['experience_details']\n",
    "    ])\n",
    "\n",
    "    projects_details = \"\\n\".join([\n",
    "        f\"Created a project titled {proj['name']}. Description of the Project:\\n  - \" +\n",
    "        '\\n  - '.join(proj['description']) + \"\\n\"\n",
    "        for proj in applicant_data['projects']\n",
    "    ])\n",
    "\n",
    "    skills_details = f\"\"\"\n",
    "**Applicant's Skills:**\n",
    "Certifications: {', '.join(applicant_data['certifications'])}\n",
    "Programming Languages: {', '.join(applicant_data['skills']['programing_languages'])}\n",
    "Databases: {', '.join(applicant_data['skills']['databases'])}\n",
    "Frameworks: {', '.join(applicant_data['skills']['frameworks'])}\n",
    "Tools: {', '.join(applicant_data['skills']['tools'])}\n",
    "Cloud: {', '.join(applicant_data['skills']['cloud'])}\n",
    "\"\"\"\n",
    "\n",
    "    job_description_section = f\"\"\"\n",
    "**Job Description:**\n",
    "{job_description}\n",
    "\"\"\"\n",
    "\n",
    "    cover_letter_requirements = \"\"\"\n",
    "**Cover Letter Requirements:**\n",
    "- Address the letter to the appropriate hiring manager or use a generic salutation if not specified.\n",
    "- Do not include a section for the company's address, city, state or zip code.\n",
    "- Introduce the applicant and express enthusiasm for the position.\n",
    "- Highlight relevant education, experience, skills, and projects that match the job requirements.\n",
    "- Explain why the applicant is a good fit for the role and the company.\n",
    "- Conclude with a polite call to action and sign off appropriately.\n",
    "- Maintain a professional and formal tone throughout.\n",
    "- The letter should be around 400-500 words.\n",
    "- It is very important to the applicant's career that the cover letter contents are the most relevant to the job description in the applicants information provided.\n",
    "\n",
    "Please write the cover letter below:\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Using the applicant's information and the job description provided, write a personalized and professional cover letter.\n",
    "\n",
    "---\n",
    "Today's date is: {datetime.now().date()}\n",
    "{personal_info}\n",
    "{education_details}\n",
    "**Applicant's Work Experience Details:**\n",
    "{experience_details}\n",
    "**Applicant's Personal Projects:**\n",
    "{projects_details}\n",
    "{skills_details}\n",
    "\n",
    "---\n",
    "\n",
    "{job_description_section}\n",
    "\n",
    "---\n",
    "\n",
    "{cover_letter_requirements}\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "prompt = create_prompt(applicant_data, job_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the applicant's information and the job description provided, write a personalized and professional cover letter.\n",
      "\n",
      "---\n",
      "Today's date is: 2024-09-02\n",
      "\n",
      "**Applicant's Personal Information:**\n",
      "Name: Bhavya Pranav Tandra\n",
      "Email: tandra.b@northeastern.edu\n",
      "Phone Number: +1 6179062441\n",
      "\n",
      "\n",
      "**Applicant's Education Details:**\n",
      "Education: Masters in Artificial Intelligence from Northeastern University with 3.76/4.00 GPA, Bachelors in Computer Science from Malla Reddy College of Engineering and Technology with 8.43/10.00 GPA\n",
      "Course Work: Large Language Models, Deep Learning, Machine Learning, Foundations in Artificial Intelligence, Artificial Intelligence for Human Computer Interaction, Algorithms, Programming Design Paradigm, Data Warehousing and Data Mining, Cloud Computing, Linux, Big Data Analytics, Data Visualization, Parallel and Distributed Computing, Database Management Systems\n",
      "\n",
      "**Applicant's Work Experience Details:**\n",
      "Worked as Senior Analyst (Data Engineer) at Capgemini where the job responsibilities include:\n",
      "  - Architected and implemented a data processing pipeline using AWS tools, integrating data from multiple sources into a unified structure. Utilized AWS S3 for storage, Glue for ETL, and Redshift for warehousing, ensuring accuracy and compliance.\n",
      "  - Designed and executed multi-stage data workflows, including ingestion into S3, transformation with PySpark on AWS EMR, and integration into Redshift. Used AWS Athena for SQL querying and validation, enhancing quality and efficiency.\n",
      "  - Developed and enforced data quality checks and error handling, leveraging AWS services to boost data integrity. Implemented automated monitoring and reporting with AWS MWAA, reducing manual interventions and improving reliability.\n",
      "  - Skills Acquired: AWS, AWS S3, AWS Glue, AWS Redshift, AWS Elastic Map-Reduce, AWS Athena, AWS MWAA, PySpark\n",
      "\n",
      "Worked as Data Engineering Intern at Capgemini where the job responsibilities include:\n",
      "  - Engineered and optimized end-to-end data pipelines using Azure Data Factory and Azure DataBricks, processing a decade-long dataset. Utilized PySpark for scalable data transformations and integrated with ADLS Gen2 for efficient data storage.\n",
      "  - Performed complex data manipulations and analysis with Azure SQL Database, applying advanced filtering and transformations to uncover trends and contributing factors, and used Azure Synapse Analytics for seamless data integration and querying.\n",
      "  - Developed interactive dashboards and detailed reports using Power BI and Tableau, visualizing trends and key factors in the dataset.\n",
      "  - Skills Acquired: Microsoft Azure, Azure Data Factory, Azure DataBricks, MS-SQL, ADLS Gen2, Azure SQL Database, Azure Synapse Analytics, Power BI, Tableu\n",
      "\n",
      "**Applicant's Personal Projects:**\n",
      "Created a project titled Google’s Isolated American Sign Language Recognition. Description of the Project:\n",
      "  - Utilized multiple models, including Transformer Encoder, LSTM, GRU, RNN, and traditional machine learning models such as SVM and KNN, to classify Mediapipe Landmark data files from video streams into 250 distinct 'sign' classes.\n",
      "  - Conducted a comprehensive comparative analysis to evaluate the performance of each model in action recognition\n",
      "  - Conducted extensive hyperparameter tuning using Optuna across a diverse range of values for all models, ensuring each model was trained with its optimal hyperparameters.\n",
      "  - Executed thorough data cleaning and processing procedures to significantly reduce the dataset size from 56GB to 12GB to facilitate model training within the constraints of limited available resources, ensuring a more streamlined and resource-efficient training process.\n",
      "\n",
      "Created a project titled Image Restoration using Multi Stage Progressive Image Restoration Model. Description of the Project:\n",
      "  - Revitalized images by addressing a spectrum of degradations, spanning from noise and motion-blur to rain artifacts, employing the state-of-the-art MPRNET model.\n",
      "  - Leveraged specific datasets, including GoPro for deblurring, SIDD for denoising, and synthetic rain datasets for deraining, while implementing a diverse set of data augmentations, such as random cropping, flipping, rotating, and adjustments to gamma and saturation, effectively expanding the dataset for more robust model training.\n",
      "  - Engineered a sophisticated 3-stage network architecture, with the initial two stages adopting a U-Net structure to grasp contextual information, and the third stage meticulously designed to preserve the original resolution of the image, ensuring the retention of intricate details throughout the training process, thereby maximizing the model's capacity to restore images with enhanced fidelity and clarity across various degradation scenarios.\n",
      "\n",
      "Created a project titled ImageManipulation Application using JAVA. Description of the Project:\n",
      "  - Developed a comprehensive Image Manipulation software with blur, filter, sepia, greyscale and more abilites exclusively using fundamental JDK components.\n",
      "  - Expertly applied design patterns, including the builder and command design patterns, and diligently followed MVC principles to create a highly scalable application while strictly adhering to SOLID principles.\n",
      "  - Emphasized modularity and scalability without compromising functionality or code readability throughout the project.\n",
      "\n",
      "Created a project titled Celeb Face Generator. Description of the Project:\n",
      "  - Implemented a complex Denoising Diffusion Probabilistic Model, as described in the prominent research paper, using PyTorch.\n",
      "  - Trained the model on the CelebAHQ Dataset to generate new, high-quality images.\n",
      "  - Demonstrated advanced skills in Python, machine learning, and image processing techniques.\n",
      "  - Successfully created a model capable of generating realistic human face images, showcasing the potential in generative modeling and AI-driven image synthesis.\n",
      "\n",
      "Created a project titled English to Italian Translation using Transformer. Description of the Project:\n",
      "  - Developed a Transformer model from scratch for machine translation based on ”Attention is All You Need” by Vaswani et al., utilizing self-attention mechanisms for efficient language translation.\n",
      "  - Enabled easy customization for various language pairs through a configurable system, enhancing the model's adaptability for different linguistic datasets.\n",
      "  - Utilized PyTorch for all aspects of the model's lifecycle, including construction, training, and inference, leveraging GPU acceleration for improved performance.\n",
      "  - Implemented a greedy decoding strategy for translation, providing a straightforward and effective approach for text translation from source to target languages.\n",
      "  - Designed a dynamic bilingual dataset loader to facilitate the efficient loading and preprocessing of bilingual sentence pairs, optimizing for computational efficiency and memory usage.\n",
      "\n",
      "Created a project titled Fine-tuning Seq2Seq Models for Text Summarization on Conversational Data. Description of the Project:\n",
      "  - Implemented a text summarization pipeline using the Samsum Corpus, focusing on generating concise summaries from conversational text.\n",
      "  - Conducted Exploratory Data Analysis (EDA) to understand dataset characteristics, including dialogue and summary lengths, using Python's pandas and matplotlib libraries.\n",
      "  - Utilized pre-trained models (BART, T5, and Pegasus) for initial summarization attempts, identifying limitations in handling conversational data.\n",
      "  - Fine-tuned the BART-large-CNN model on the Samsum Corpus to improve summarization performance, adapting the model to better understand conversational semantics.\n",
      "  - Evaluated model performance using ROUGE scores, demonstrating significant improvement in summarization quality post fine-tuning.\n",
      "  - Conducted both quantitative and qualitative analyses to compare pre-trained and fine-tuned model outputs, showcasing the enhanced coherence and relevance of generated summaries.\n",
      "\n",
      "Created a project titled Generating High Quality Stories by Fine-Tuning and Aligning Phi-2. Description of the Project:\n",
      "  - Pioneering the development of StoryGen, a novel Small Language Model based on Phi-2, aimed at revolutionizing storytelling by employing unique fine-tuning and instruct-tuning techniques on a diverse Stories Dataset to capture intricate narrative nuances.\n",
      "  - Leveraging advanced AI techniques, including QLoRA for Parameter Efficient Fine-Tuning, to refine the model's capacity for generating compelling narratives that resonate deeply with audiences.\n",
      "  - Implemented Reinforcement Learning from Human Feedback (RLHF) to significantly elevate story quality, coherence, and narrative complexity, showcasing potential through preliminary human evaluators' ratings and user engagement metrics.\n",
      "  - Innovated with alignment techniques such as Direct Preference Optimization (DPO) and Self-Rewarding Language Models, to align the generated stories closely with human preferences and storytelling standards.\n",
      "  - Conducted comprehensive evaluations through a blend of human ratings and quantitative metrics, aiming to demonstrate StoryGen's superior narrative generation capabilities compared to conventional models.\n",
      "  - Used HuggingFace's Transformers, TRL, PEFT and BitsAndBytes, Accelerate and DeepSpeed.\n",
      "\n",
      "Created a project titled Lightweight Visual Question Answering using Gemma2B-it. Description of the Project:\n",
      "  - Spearheading the development of a Lightweight Visual Question Answering (LVQA) system, leveraging the Gemma 2B Instruct model, into LLaVA framework to optimize efficiency and scalability.\n",
      "  - Employing advanced natural language processing and image encoding techniques, integrating CLIP-ViT-L-336px with Gemma2B-it, to refine visual question interpretation capabilities.\n",
      "  - Pretraining multimodal Large Language and Vision model on text-image pairs, and Visual Instruction Tuning on synthetic Visual Instruct Data.\n",
      "  - Utilizing state-of-the-art tools such as PyTorch, TensorFlow, and Hugging Face Transformers to facilitate model development and performance benchmarking.\n",
      "  - Benchmark evaluations highlight the LVQA system's potential to surpass existing models in AI efficiency and accessibility, indicating a groundbreaking step forward in the field.\n",
      "\n",
      "\n",
      "**Applicant's Skills:**\n",
      "Certifications: Coursera- Deep Learning Specialization, Coursera- AI for Medicine Specialization, IBM Data Visualization, Google Data Analytics Professional Certificate, Coursera- MLOps Specialization by Duke University\n",
      "Programming Languages: Java, Python, C, C++, R, MATLAB\n",
      "Databases: MongoDB, ElasticSearch, Firebase, MySQL, Cassandra, Oracle, PostgreSQL, MSSQL, SQLite, Amazon DynamoDB, ChromaDB, Pinecone, FAISS\n",
      "Frameworks: PyTorch, TensorFlow, Keras, HuggingFace, Langchain, OpenAI, VertexAI, OpenCV, NLTK, Scikit-Learn, NumPy, Pandas, SciPy, PySpark\n",
      "Tools: Git, MLFlow, DVC, AirFlow, Docker, GitHub Actions, Kubernetes, Grafana, Terraform\n",
      "Cloud: Amazon Sagemaker, AWS Lambda, AWS EMR, AWS S3, AWS Redshift, Amazon RDS, Amazon Cloudwatch, Amazon EC2, Amazon ECS, Amazon EKS, Azure DevOps, Azure MachineLearning, Azure DataBricks, Azure Synapse Analytics, Azure Data Lake, Azure Functions, Azure Kubernetes Services, Azure Event Hubs, Azure Cognitive Servieces\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "**Job Description:**\n",
      "About the job Location: Remote Type: Internship Duration: 3-6 months Start Date: October 1st, 2024   About IAN: IAN (Intelligent Agent Network) is at the forefront of ethical AI innovation. By seamlessly connecting multiple AI agents, IAN provides clear, unbiased insights for businesses and individuals. Join us as we build a smarter, fairer AI ecosystem that drives real-world impact.   Role Overview: As an AI Engineer Intern at IAN, you will dive deep into the world of AI, working on multi-agent systems and advanced machine learning models. This role offers hands-on experience in AI development, focusing on integrating diverse AI agents and addressing biases in AI systems. If you’re passionate about ethical AI and eager to make a difference, this role is for you.   Key Responsibilities:  Collaborate with the engineering team to design, develop, and test AI models and algorithms. Assist in integrating multiple AI agents into a cohesive and powerful system. Conduct research on AI biases and help develop mitigation strategies. Work on data preprocessing, model training, and optimization of AI performance. Participate in code reviews and contribute to technical documentation. Present your work and findings to the team, showcasing the impact of your contributions. Qualifications:  Currently pursuing or recently completed a degree in Computer Science, Data Science, AI, Machine Learning, or a related field. Strong understanding of machine learning algorithms and AI concepts. Experience with programming languages such as Python, TensorFlow, PyTorch, or similar AI frameworks. Familiarity with data preprocessing, model training, and evaluation techniques. A passion for ethical AI and a drive to solve real-world problems using AI technology. Ability to work effectively in a fast-paced, collaborative environment. Bonus Skills:  Experience with natural language processing (NLP) models. Familiarity with cloud platforms (AWS, Google Cloud, etc.). Experience in hackathons, side projects, or launching a startup. Proven track record of building projects independently or as part of a team. Why Join IAN?  Gain hands-on experience in the ethical AI industry with real-world impact. Work alongside a dynamic team of engineers, data scientists, and AI researchers. Collaborate on meaningful projects with the potential to shape the future of AI. Flexible work hours with the opportunity to work remotely. Potential for full-time employment based on performance.   Salary  $120,000 USD   How to Apply: Submit your resume, a brief cover letter, and any relevant project work or portfolio to brett@cyphae.com. Applications are reviewed on a rolling basis, so we encourage you to apply early.   Join IAN and be a part of revolutionizing AI.\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "**Cover Letter Requirements:**\n",
      "- Address the letter to the appropriate hiring manager or use a generic salutation if not specified.\n",
      "- Do not include a section for the company's address, city, state or zip code.\n",
      "- Introduce the applicant and express enthusiasm for the position.\n",
      "- Highlight relevant education, experience, skills, and projects that match the job requirements.\n",
      "- Explain why the applicant is a good fit for the role and the company.\n",
      "- Conclude with a polite call to action and sign off appropriately.\n",
      "- Maintain a professional and formal tone throughout.\n",
      "- The letter should be around 400-500 words.\n",
      "- It is very important to the applicant's career that the cover letter contents are the most relevant to the job description in the applicants information provided.\n",
      "\n",
      "Please write the cover letter below:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=key)\n",
    "\n",
    "def generate_cover_letter(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert career consultant and professional writer, who specializes in drafting cover letters.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "        temperature=0.4,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "cover_letter = generate_cover_letter(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Bhavya Pranav Tandra**  \n",
      "tandra.b@northeastern.edu  \n",
      "+1 6179062441  \n",
      "September 2, 2024  \n",
      "\n",
      "Hiring Manager  \n",
      "IAN (Intelligent Agent Network)  \n",
      "\n",
      "Dear Hiring Manager,\n",
      "\n",
      "I am writing to express my enthusiasm for the AI Engineer Intern position at IAN, as advertised. With a Master’s degree in Artificial Intelligence from Northeastern University and extensive hands-on experience in data engineering and machine learning, I am excited about the opportunity to contribute to your mission of building an ethical AI ecosystem.\n",
      "\n",
      "During my time as a Senior Analyst (Data Engineer) at Capgemini, I architected and implemented a robust data processing pipeline utilizing AWS tools, ensuring data accuracy and compliance. This experience honed my skills in integrating diverse data sources, which aligns perfectly with IAN's focus on connecting multiple AI agents. Additionally, my internship at Capgemini allowed me to engineer and optimize end-to-end data pipelines using Azure technologies, further enhancing my proficiency in cloud platforms, a bonus skill mentioned in your job description.\n",
      "\n",
      "My academic coursework has provided me with a solid foundation in machine learning algorithms and AI concepts, particularly in areas such as Deep Learning and Natural Language Processing (NLP). I have developed several projects that demonstrate my ability to work with advanced AI models. For instance, I created a Transformer model for language translation and fine-tuned Seq2Seq models for text summarization, which involved extensive data preprocessing and model optimization. These projects not only reflect my technical skills but also my passion for ethical AI, as I continuously strive to address biases and improve model performance.\n",
      "\n",
      "At IAN, I am particularly drawn to the opportunity to collaborate with a dynamic team to design and test AI models while conducting research on AI biases. My project titled \"Google’s Isolated American Sign Language Recognition\" involved a comparative analysis of various machine learning models, where I implemented rigorous data cleaning and hyperparameter tuning to enhance model performance. This experience has equipped me with the necessary skills to contribute to your research on bias mitigation strategies effectively.\n",
      "\n",
      "I am excited about the prospect of working in a fast-paced, collaborative environment like IAN, where I can leverage my programming skills in Python, TensorFlow, and PyTorch to make a meaningful impact. I am eager to bring my experience in building and optimizing AI systems to your team and contribute to projects that have real-world implications.\n",
      "\n",
      "Thank you for considering my application. I look forward to the opportunity to discuss how my background, skills, and enthusiasm for ethical AI align with the goals of IAN. I am excited about the possibility of contributing to your innovative projects and hope to hear from you soon.\n",
      "\n",
      "Sincerely,  \n",
      "Bhavya Pranav Tandra  \n"
     ]
    }
   ],
   "source": [
    "print(cover_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "def format_cover_letter(cover_letter_text, template_path='templates', template_file='cover_letter_template.html'):\n",
    "    env = Environment(loader=FileSystemLoader(template_path))\n",
    "    template = env.get_template(template_file)\n",
    "    rendered_letter = template.render(cover_letter=cover_letter_text.replace('\\n', '<br>').replace('\\n\\n', '</p><p>').replace('**',''))\n",
    "    return rendered_letter\n",
    "\n",
    "formatted_cover_letter = format_cover_letter(cover_letter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('res/cover_letter.html', mode='w') as file:\n",
    "    file.write(formatted_cover_letter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fpdf in ./myenv/lib/python3.10/site-packages (1.7.2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from fpdf import FPDF\n",
    "\n",
    "def create_pdf(cover_letter, output_path='res/cover_letter.pdf'):\n",
    "    pdf = FPDF(format='letter')\n",
    "    pdf.add_page()\n",
    "\n",
    "    # Set font\n",
    "    pdf.set_font(\"Arial\", size=11)\n",
    "\n",
    "    # Define margins\n",
    "    pdf.set_left_margin(15)\n",
    "    pdf.set_right_margin(15)\n",
    "    pdf.set_top_margin(20)\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "\n",
    "    # Add a title or header\n",
    "    pdf.set_font(\"Arial\", 'B', size=11)\n",
    "    \n",
    "\n",
    "    # Add multi-line text\n",
    "    pdf.set_font(\"Arial\", size=11)\n",
    "    text = cover_letter\n",
    "    text = text.replace('’', \"'\").replace('“', '\"').replace('”', '\"').replace('–', '-').replace('**','')\n",
    "    pdf.multi_cell(0, 5, text)  # Adjust line height as needed\n",
    "\n",
    "    # Save the PDF\n",
    "    pdf.output(output_path)\n",
    "\n",
    "create_pdf(cover_letter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
